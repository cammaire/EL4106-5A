{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQId5PCpMRai"
      },
      "source": [
        "Grupo 5A: Nicolás Camousseight / Camila Maire\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egW5y_VHNZNP"
      },
      "source": [
        "# Red convolucional para detección temprana de supernovas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njEp78MayxtD"
      },
      "outputs": [],
      "source": [
        "#Librerías\n",
        "\n",
        "import pickle as pkl\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, make_scorer, accuracy_score, classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
        "\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "#import tensorflow as tf\n",
        "#from tensorflow import keras\n",
        "#from tensorflow.keras import layers, models\n",
        "#from tensorflow.keras.utils import to_categorical\n",
        "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "#from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cISOcem3yxtJ"
      },
      "source": [
        "## Red convolucional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOywbpmPyxtK"
      },
      "source": [
        "### Accedemos a la data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5vNCNKpQnG8",
        "outputId": "47845e18-84dc-4fc5-9058-c746db6d8886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDNccFL-yxtN",
        "outputId": "8f080c8c-deeb-4656-85d1-6cc0326b9b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys de pkl dict_keys(['Train', 'Validation', 'Test'])\n",
            "keys de Test dict_keys(['images', 'labels', 'features'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Función para cargar el contenido del archivo .pkl\n",
        "def load_pickle_data(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        data = pkl.load(f)\n",
        "    return data\n",
        "\n",
        "# Cargar el archivo\n",
        "filename = '/content/drive/My Drive/stamps.pkl'\n",
        "data_procesada = load_pickle_data(filename)\n",
        "\n",
        "# Visualizar el contenido\n",
        "print('keys de pkl', data_procesada.keys())\n",
        "print('keys de Test',data_procesada['Validation'].keys())\n",
        "data_procesada['Validation']['labels']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IecRy0qyxtS"
      },
      "source": [
        "### Generamos las variables para Train, Val y Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeTWqHJPyxtT",
        "outputId": "725a7c0c-3733-4714-e737-ad568b3e26cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tipo de dato de Train: <built-in method type of Tensor object at 0x7fc2094f3880>\n",
            "forma de Train: torch.Size([72710, 3, 63, 63])\n",
            "forma de Validation: torch.Size([500, 3, 63, 63])\n",
            "forma de Test: torch.Size([500, 3, 63, 63])\n"
          ]
        }
      ],
      "source": [
        "def preparar_imagenes_para_modelo(data_dict, key_principal='Train', key_imagenes='images'):\n",
        "    # Extraer imágenes del diccionario\n",
        "    imagenes = data_dict[key_principal][key_imagenes]\n",
        "\n",
        "    # Cambiar el orden de las dimensiones a [canales, altura, ancho]\n",
        "    imagenes = np.transpose(imagenes, (0, 3, 1, 2))\n",
        "\n",
        "    # Convertir a tensor de PyTorch\n",
        "    imagenes_tensor = torch.tensor(imagenes, dtype=torch.float32)\n",
        "\n",
        "    return imagenes_tensor\n",
        "\n",
        "# Preparar las imágenes para el modelo\n",
        "Train = preparar_imagenes_para_modelo(data_procesada)\n",
        "Val = preparar_imagenes_para_modelo(data_procesada, key_principal='Validation')\n",
        "Test = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
        "\n",
        "print('tipo de dato de Train:', Train.type)\n",
        "print('forma de Train:', Train.shape)\n",
        "print('forma de Validation:', Val.shape)\n",
        "print('forma de Test:', Test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGpetaXiyxtU"
      },
      "source": [
        "### Ahora extraemos las características"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfqifyvGyxtV"
      },
      "outputs": [],
      "source": [
        "def extraer_etiquetas(data_dict, key_principal='Train', key_etiquetas='labels'):\n",
        "    # Extraer etiquetas del diccionario\n",
        "    etiquetas = data_dict[key_principal][key_etiquetas]\n",
        "    # De etiquetas de diccionario, pasamos a tensor de PyTorch, pindices de clase\n",
        "    etiquetas_tensor = torch.tensor(etiquetas, dtype=torch.long)\n",
        "    return etiquetas_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Puulk9gxyxtW"
      },
      "source": [
        "### Extracción de metadata:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm_AVrffyxtX",
        "outputId": "0cdd0ef2-0fff-4511-8d90-17039718d5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forma de metadata_train: torch.Size([72710, 26])\n",
            "forma de metadata_val: torch.Size([500, 26])\n",
            "forma de metadata_test: torch.Size([500, 26])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 7.7878e-01, -6.2952e-01,  8.8655e-02,  ..., -1.4237e-01,\n",
              "         -2.2997e-01, -1.3517e-01],\n",
              "        [ 7.6781e-01, -6.2212e-01,  1.2532e+00,  ..., -2.5771e-01,\n",
              "         -3.8337e-01,  9.2198e-02],\n",
              "        [ 7.5052e-01, -6.1387e-01, -1.0466e+00,  ..., -3.4228e-01,\n",
              "         -3.9726e-01,  3.2286e-03],\n",
              "        ...,\n",
              "        [-1.8353e+00,  2.9948e+00,  8.8655e-02,  ...,  1.6006e-01,\n",
              "         -2.2666e-01, -5.8001e-01],\n",
              "        [-1.8213e+00,  1.4213e+00,  8.8655e-02,  ...,  5.7526e-01,\n",
              "         -3.6618e-01,  6.2541e-02],\n",
              "        [ 7.7110e-01,  1.4931e+00,  1.2525e+00,  ...,  3.3484e+00,\n",
              "         -1.3608e-01, -4.7127e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def extraer_metadata(data_dict, key_principal='Train', key_metadata='features'):\n",
        "    # Extraer características de metadatos del diccionario\n",
        "    metadata = data_dict[key_principal][key_metadata]\n",
        "    # Convertir explícitamente a float32\n",
        "    metadata = metadata.astype(np.float32)\n",
        "    metadata_tensor = torch.tensor(metadata, dtype=torch.float32)\n",
        "    return metadata_tensor\n",
        "\n",
        "# Extraer características de metadatos para los conjuntos de datos\n",
        "metadata_train = extraer_metadata(data_procesada)\n",
        "metadata_val = extraer_metadata(data_procesada, key_principal='Validation')\n",
        "metadata_test = extraer_metadata(data_procesada, key_principal='Test')\n",
        "\n",
        "print('forma de metadata_train:', metadata_train.shape)\n",
        "print('forma de metadata_val:', metadata_val.shape)\n",
        "print('forma de metadata_test:', metadata_test.shape)\n",
        "\n",
        "metadata_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZUMeti5yxtY"
      },
      "source": [
        "## Modelo convolucional, incorporando invariancia rotacional. Implementar lo realizado en el paper de referencia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiaTv8KmlN6-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, dropout_p):\n",
        "        super(CNNModel, self).__init__()\n",
        "        # Zero padding para evitar perder datos de las imágenes después de las convoluciones\n",
        "        self.padding = nn.ZeroPad2d(4)  # Ajustado a 5 para compensar el mayor tamaño del kernel\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "        # Bloques de convolución\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=0),  # Tamaño del kernel ajustado a 5, padding ajustado\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),  # Tamaño del kernel ajustado a 5, padding ajustado\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Tamaño del kernel ajustado a 5, padding ajustado\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Tamaño del kernel ajustado a 5, padding ajustado\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Tamaño del kernel ajustado a 5, padding ajustado\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "\n",
        "        # MLP 1\n",
        "        self.mlp1 = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16384, 64),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Normalización de Batch para Metadata\n",
        "        self.metadata_bn = nn.BatchNorm1d(26)\n",
        "\n",
        "        self.mlp2 = nn.Sequential(\n",
        "            nn.Dropout(p=self.dropout_p),\n",
        "            nn.Linear(90, 64),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.mlp3 = nn.Sequential(\n",
        "            nn.Linear(64, 5),\n",
        "        )\n",
        "\n",
        "    def _get_conv_output(self, x):\n",
        "        x = self.padding(x)\n",
        "        x = self.rotate_input(x)\n",
        "        x = self.conv_blocks(x)\n",
        "        return int(np.prod(x.shape[1:]))\n",
        "\n",
        "    def rotate_input(self, x):\n",
        "        # Rotaciones para la invariancia\n",
        "        x_90 = torch.rot90(x, 1, [2, 3])\n",
        "        x_180 = torch.rot90(x, 2, [2, 3])\n",
        "        x_270 = torch.rot90(x, 3, [2, 3])\n",
        "        return torch.cat([x, x_90, x_180, x_270], dim=0)\n",
        "\n",
        "    def cyclic_pooling(self, x):\n",
        "        B = x.size(0) // 4\n",
        "        return (x[:B] + x[B:2*B] + x[2*B:3*B] + x[3*B:]) / 4.0\n",
        "\n",
        "    def forward(self, x, metadata):\n",
        "        # Verificación de dimensiones de metadatos\n",
        "        if metadata.dim() != 2:\n",
        "            raise ValueError(\"La metadata debe ser un tensor de 2 dimensiones\")\n",
        "        # Zero padding\n",
        "        x = self.padding(x)\n",
        "        # Rotación de la entrada y bloques de convolución\n",
        "        x_rotated = self.rotate_input(x)\n",
        "        x_rotated = self.conv_blocks(x_rotated)\n",
        "        # Primera capa fully connected\n",
        "        x_rotated_dense = self.mlp1(x_rotated)\n",
        "        # Pooling Cíclico\n",
        "        x_pooled = self.cyclic_pooling(x_rotated_dense)\n",
        "        # Normalización de metadatos\n",
        "        normalized_metadata = self.metadata_bn(metadata)\n",
        "        # Concatenación de características de imagen y metadatos\n",
        "        combined_features = torch.cat([x_pooled, normalized_metadata], dim=1)\n",
        "\n",
        "        # Pasar por la segunda parte del MLP\n",
        "        out = self.mlp2(combined_features)\n",
        "        out = self.mlp3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def calculate_linear_input_size(self, input_size):\n",
        "        \"\"\"\n",
        "        Calculates the input size for the first linear layer.\n",
        "\n",
        "        This function takes a sample input and passes it through the\n",
        "        convolutional layers to determine the output size, which will\n",
        "        be the input size for the linear layer.\n",
        "        \"\"\"\n",
        "        x = torch.randn(1, *input_size)  # Create a dummy input\n",
        "        x = self.padding(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        self.linear_input_size = x.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iU1RyCOqyxtZ"
      },
      "source": [
        "## Función de entrenamiento y visualización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CRgh6VLyxta"
      },
      "outputs": [],
      "source": [
        "def show_curves(curves):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
        "    fig.set_facecolor('white')\n",
        "    epochs = np.arange(len(curves[\"val_loss\"])) + 1\n",
        "\n",
        "    ax[0].plot(epochs, curves['val_loss'], label='validation')\n",
        "    ax[0].plot(epochs, curves['train_loss'], label='training')\n",
        "    ax[0].set_xlabel('Epoch')\n",
        "    ax[0].set_ylabel('Loss')\n",
        "    ax[0].set_title('Loss evolution during training')\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].plot(epochs, curves['val_acc'], label='validation')\n",
        "    ax[1].plot(epochs, curves['train_acc'], label='training')\n",
        "    ax[1].set_xlabel('Epoch')\n",
        "    ax[1].set_ylabel('Accuracy')\n",
        "    ax[1].set_title('Accuracy evolution during training')\n",
        "    ax[1].legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgtvktteyxta"
      },
      "outputs": [],
      "source": [
        "def perdida_regularizada_entropia(salidas, objetivos, beta=0.1):\n",
        "    # Pérdida de entropía cruzada\n",
        "    perdida_ec = F.cross_entropy(salidas, objetivos, reduction='mean')\n",
        "    # Calcula la entropía de las probabilidades\n",
        "    probabilidades = F.softmax(salidas, dim=1)\n",
        "    entropia = -torch.sum(probabilidades * torch.log(probabilidades + 1e-9), dim=1)\n",
        "    entropia_reg = torch.mean(entropia)\n",
        "    # Pérdida combinada\n",
        "    perdida_total = perdida_ec - beta * entropia_reg\n",
        "\n",
        "    return perdida_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxIkQSp_yxtc"
      },
      "outputs": [],
      "source": [
        "def train_step(x_batch, y_batch, metadata_batch, model, optimizer, criterion, beta):\n",
        "    # Predicción\n",
        "    y_predicted = model(x_batch, metadata_batch)\n",
        "    # Cálculo de la pérdida\n",
        "    loss = criterion(y_predicted, y_batch, beta=beta)\n",
        "    # Actualización de parámetros\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return y_predicted, loss\n",
        "\n",
        "\n",
        "def evaluate(data_loader, model, criterion, use_gpu):\n",
        "    cumulative_loss = 0\n",
        "    cumulative_predictions = 0\n",
        "    data_count = 0\n",
        "\n",
        "    for x_val, y_val, metadata_val in data_loader:\n",
        "        if use_gpu:\n",
        "            x_val = x_val.cuda()\n",
        "            y_val = y_val.cuda()\n",
        "            metadata_val = metadata_val.cuda()\n",
        "        y_predicted = model(x_val, metadata_val)\n",
        "        loss = criterion(y_predicted, y_val)\n",
        "        class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
        "\n",
        "        cumulative_predictions += (y_val == class_prediction).sum().item()\n",
        "        cumulative_loss += loss.item()\n",
        "        data_count += y_val.shape[0]\n",
        "    val_acc = cumulative_predictions / data_count\n",
        "    val_loss = cumulative_loss / len(data_loader)\n",
        "\n",
        "    return val_acc, val_loss\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    train_data_images,\n",
        "    train_data_labels,\n",
        "    train_data_metadata,\n",
        "    val_data_images,\n",
        "    val_data_labels,\n",
        "    val_data_metadata,\n",
        "    epochs,\n",
        "    criterion,\n",
        "    batch_size,\n",
        "    lr,\n",
        "    use_gpu,\n",
        "    beta=0.1,\n",
        "    patience=12,\n",
        "    val_batch_size=None\n",
        "):\n",
        "    if use_gpu:\n",
        "        model.cuda()\n",
        "\n",
        "    # Definición de dataloader\n",
        "    train_dataset = torch.utils.data.TensorDataset(train_data_images, train_data_labels, train_data_metadata)\n",
        "    val_dataset = torch.utils.data.TensorDataset(val_data_images, val_data_labels, val_data_metadata)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=use_gpu)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=val_batch_size or len(val_data_images), shuffle=False, pin_memory=use_gpu)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    curves = {\n",
        "        \"train_acc\": [],\n",
        "        \"val_acc\": [],\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "    }\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\rEpoch {epoch + 1}/{epochs}\")\n",
        "        cumulative_train_loss = 0\n",
        "        cumulative_train_corrects = 0\n",
        "        train_loss_count = 0\n",
        "        train_acc_count = 0\n",
        "\n",
        "        # Entrenamiento del modelo\n",
        "        model.train()\n",
        "\n",
        "        for i, (x_batch, y_batch, metadata_batch) in enumerate(train_loader):\n",
        "            if use_gpu:\n",
        "                x_batch = x_batch.cuda()\n",
        "                y_batch = y_batch.cuda()\n",
        "                metadata_batch = metadata_batch.cuda()\n",
        "\n",
        "            y_predicted, loss = train_step(x_batch, y_batch, metadata_batch, model, optimizer, criterion, beta)\n",
        "\n",
        "            cumulative_train_loss += loss.item()\n",
        "            train_loss_count += 1\n",
        "            train_acc_count += y_batch.shape[0]\n",
        "\n",
        "            # Calculamos número de aciertos\n",
        "            class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
        "            cumulative_train_corrects += (y_batch == class_prediction).sum().item()\n",
        "\n",
        "            if (i + 1) % 117 == 0:\n",
        "                print(f\"Iteration {i + 1} - Batch {i + 1}/{len(train_loader)} - Train loss: {cumulative_train_loss / train_loss_count}, Train acc: {cumulative_train_corrects / train_acc_count}\")\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_acc, val_loss = evaluate(val_loader, model, criterion, use_gpu)\n",
        "\n",
        "        print(f\"Val loss: {val_loss}, Val acc: {val_acc}\")\n",
        "\n",
        "        train_loss = cumulative_train_loss / train_loss_count\n",
        "        train_acc = cumulative_train_corrects / train_acc_count\n",
        "        curves[\"train_acc\"].append(train_acc)\n",
        "        curves[\"val_acc\"].append(val_acc)\n",
        "        curves[\"train_loss\"].append(train_loss)\n",
        "        curves[\"val_loss\"].append(val_loss)\n",
        "\n",
        "        # Lógica de early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        if epochs_without_improvement == patience:\n",
        "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement after {patience} epochs.\")\n",
        "            break\n",
        "\n",
        "    print(f\"Tiempo total de entrenamiento: {time.perf_counter() - t0:.4f} [s]\")\n",
        "    model.cpu()\n",
        "\n",
        "    return curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ki1qtYZyxtd"
      },
      "source": [
        "### Buscamos hiperparámetros con funciones de sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEM7tYeGyxtd"
      },
      "outputs": [],
      "source": [
        "# Dividir las imágenes y metadatos en conjuntos de entrenamiento y validación\n",
        "Train_images = preparar_imagenes_para_modelo(data_procesada)\n",
        "Val_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Validation')\n",
        "Train_labels = extraer_etiquetas(data_procesada)\n",
        "Val_labels = extraer_etiquetas(data_procesada, key_principal='Validation')\n",
        "# Extraer características de metadatos para los conjuntos de datos\n",
        "metadata_train = extraer_metadata(data_procesada)\n",
        "metadata_val = extraer_metadata(data_procesada, key_principal='Validation')\n",
        "metadata_test = extraer_metadata(data_procesada, key_principal='Test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNZtu8fVyxtf"
      },
      "source": [
        "## Aquí empezamos el entrenamiento :) a cambiar hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldlk7XUbZqfK",
        "outputId": "9daba872-8c83-4425-8ba5-560eeeab9425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: lspci: command not found\n"
          ]
        }
      ],
      "source": [
        "!lspci | grep -i nvidia\n",
        "#Con esta línea, verificamos si hay driver de NVIDIA\n",
        "#Si existe: use_gpu=True, si no, use_gpu=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC7TRXLhyxtg"
      },
      "source": [
        "### Primera prueba de hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uPNGjVEyxtg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d06833c1-6f95-4748-ff83-7aaf2673d706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpoch 1/15\n"
          ]
        }
      ],
      "source": [
        "# Instanciación del modelo\n",
        "lr = 1e-5\n",
        "dropout_p = 0.5\n",
        "batch_size = 64\n",
        "criterion = perdida_regularizada_entropia\n",
        "epochs = 15\n",
        "model = CNNModel(dropout_p=dropout_p)\n",
        "\n",
        "curves = train_model(\n",
        "    model,\n",
        "    Train_images,\n",
        "    Train_labels,\n",
        "    metadata_train,\n",
        "    Val_images,\n",
        "    Val_labels,\n",
        "    metadata_val,\n",
        "    epochs,\n",
        "    criterion,\n",
        "    batch_size,\n",
        "    lr,\n",
        "    use_gpu=False,\n",
        "    beta=0.4,\n",
        "    patience=20\n",
        ")\n",
        "\n",
        "\n",
        "# Mostrar las curvas de entrenamiento\n",
        "show_curves(curves)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CMcNSacyxti"
      },
      "outputs": [],
      "source": [
        "def predict(model, data_loader, use_gpu):\n",
        "    all_preds = []\n",
        "    all_true = []\n",
        "\n",
        "    if use_gpu:\n",
        "        model = model.cuda()  # Transfiere el modelo a la GPU\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, metadata in data_loader:\n",
        "            if use_gpu:\n",
        "                inputs = inputs.cuda()\n",
        "                metadata = metadata.cuda()\n",
        "\n",
        "            outputs = model(inputs, metadata)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_true.extend(labels.numpy())\n",
        "\n",
        "    return all_true, all_preds\n",
        "\n",
        "# Asegúrate de tener tus datos de test preparados\n",
        "Test_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
        "Test_labels = extraer_etiquetas(data_procesada, key_principal='Test')\n",
        "metadata_test = extraer_metadata(data_procesada, key_principal='Test')\n",
        "\n",
        "# Definición de dataloader\n",
        "test_dataset = torch.utils.data.TensorDataset(Test_images, Test_labels, metadata_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "true_labels, predicted_labels = predict(model, test_loader, use_gpu=True)\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Visualizar la matriz de confusión\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['AGN', 'SN', 'VS', 'asteroid', 'bogus'])\n",
        "fig, ax = plt.subplots(figsize=(7, 7))  # Ajusta el tamaño del gráfico\n",
        "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False)  # Elimina la barra de color\n",
        "\n",
        "# Ajustes adicionales para mejorar la visualización\n",
        "ax.set_xlabel('Predicted', fontsize=12)\n",
        "ax.set_ylabel('True', fontsize=12)\n",
        "ax.grid(False)  # Elimina las líneas de la cuadrícula\n",
        "\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=45)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "report = classification_report(true_labels, predicted_labels, target_names=['AGN', 'SN', 'VS', 'asteroid', 'bogus'])\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73791rH_yxts"
      },
      "source": [
        "# Búsqueda de hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCjvJyBjyxtt"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# 1. Define los rangos de los hiperparámetros\n",
        "param_grid = {\n",
        "    'dropout_p': [0.2, 0.3, 0.4, 0.5],\n",
        "    'batch_size': [16, 32, 64],\n",
        "    'lr': [1e-4, 1e-3, 1e-2, 1e-5],\n",
        "    'beta': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
        "}\n",
        "\n",
        "# 2. Número de combinaciones a probar\n",
        "num_combinations = 15\n",
        "\n",
        "# 3. Realiza la búsqueda aleatoria\n",
        "best_val_acc = 0\n",
        "best_params = None\n",
        "results = []\n",
        "\n",
        "for i in range(num_combinations):\n",
        "    print(f\"Combinación {i + 1}/{num_combinations}\")\n",
        "\n",
        "    # Genera una combinación aleatoria de hiperparámetros\n",
        "    params = {k: random.choice(v) for k, v in param_grid.items()}\n",
        "\n",
        "    # Inicializa el modelo\n",
        "    model = CNNModel(params['dropout_p'])\n",
        "\n",
        "    # Entrena el modelo y evalúalo\n",
        "    curves = train_model(\n",
        "        model,\n",
        "        Train_images,\n",
        "        Train_labels,\n",
        "        metadata_train,\n",
        "        Val_images,\n",
        "        Val_labels,\n",
        "        metadata_val,\n",
        "        epochs=20,  # Ajustable\n",
        "        criterion=perdida_regularizada_entropia,\n",
        "        batch_size=params['batch_size'],\n",
        "        lr=params['lr'],\n",
        "        use_gpu=False,\n",
        "        beta=params['beta'],\n",
        "        patience=15\n",
        "    )\n",
        "\n",
        "\n",
        "    val_acc = max(curves['val_acc'])\n",
        "\n",
        "    # Guarda los resultados\n",
        "    results.append({\n",
        "        'params': params,\n",
        "        'val_acc': val_acc,\n",
        "        'curves': curves,\n",
        "    })\n",
        "\n",
        "    # Actualiza los mejores parámetros si es necesario\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_params = params\n",
        "\n",
        "print(\"Búsqueda completada\")\n",
        "print(f\"Mejor precisión de validación: {best_val_acc}\")\n",
        "print(f\"Mejores parámetros: {best_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kmSHk1oyxtj"
      },
      "source": [
        "### Segunda combinación de hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_mXH1SLyxtj"
      },
      "outputs": [],
      "source": [
        "# Instanciación del modelo\n",
        "lr = 5e-5\n",
        "dropout_p = 0.3\n",
        "batch_size = 64\n",
        "criterion = perdida_regularizada_entropia\n",
        "epochs = 15\n",
        "model = CNNModel(dropout_p=dropout_p)\n",
        "\n",
        "curves = train_model(\n",
        "    model,\n",
        "    Train_images,\n",
        "    Train_labels,\n",
        "    metadata_train,\n",
        "    Val_images,\n",
        "    Val_labels,\n",
        "    metadata_val,\n",
        "    epochs,\n",
        "    criterion,\n",
        "    batch_size,\n",
        "    lr,\n",
        "    use_gpu=False,\n",
        "    beta=0.3,\n",
        "    patience=20\n",
        ")\n",
        "\n",
        "# Mostrar las curvas de entrenamiento\n",
        "show_curves(curves)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6zwiv75yxtn"
      },
      "outputs": [],
      "source": [
        "# Asegúrate de tener tus datos de test preparados\n",
        "Test_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
        "Test_labels = extraer_etiquetas(data_procesada, key_principal='Test')\n",
        "metadata_test = extraer_metadata(data_procesada, key_principal='Test')\n",
        "\n",
        "# Definición de dataloader\n",
        "test_dataset = torch.utils.data.TensorDataset(Test_images, Test_labels, metadata_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "true_labels, predicted_labels = predict(model, test_loader, use_gpu=True)\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Visualizar la matriz de confusión\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2, 3, 4])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "\n",
        "report = classification_report(true_labels, predicted_labels, target_names=['AGN', 'SN', 'VS', 'asteroid', 'bogus'])\n",
        "\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}